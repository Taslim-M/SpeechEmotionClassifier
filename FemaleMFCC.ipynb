{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import scipy \n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "def convert_single_audio_to_image(audio_path, image_path, spectrogram_dimensions = (64,64)):\n",
    "    data, sr = librosa.load(audio_path, sr = 22050)\n",
    "\n",
    "    window_width =  0.025 #25 ms  window size \n",
    "    sliding = 0.01 #10ms stride \n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=sr, n_fft = int(window_width*sr), hop_length =int(sliding*sr))\n",
    "   \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    middle = int(mfccs.shape[1]/2) #we want to get the sample from the middle of the signal for 1.3 s\n",
    "    total_time = 130 #1.3 seconds\n",
    "    half_time = int(total_time/2)\n",
    "    mfccs = mfccs[:,middle-half_time:middle+half_time]\n",
    "    \n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time', hop_length =int(sliding*sr))\n",
    "    fig.savefig(image_path,bbox_inches='tight', transparent=True, pad_inches=0.0)\n",
    "    plt.close(fig)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of Wav_files in 3512= cremad\\FemaleAudioWAV\n",
      "count of Wav_files in 3512= cremad\\MFCCinit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    audio_folder_name= r\"cremad\\FemaleAudioWAV\"\n",
    "    image_folder_name=  r\"cremad\\MFCCinit\"\n",
    "    file_names = listdir(audio_folder_name)\n",
    "    print(\"count of Wav_files in {}= {}\".format(len(file_names),audio_folder_name))\n",
    "    for file_name in file_names:\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        audio_path = audio_folder_name + '\\\\' + file_name\n",
    "        spectogram_path = image_folder_name + '\\\\' + file_name.replace('.wav', '.png')\n",
    "        convert_single_audio_to_image(audio_path,spectogram_path)\n",
    "    \n",
    "    image_files= listdir(image_folder_name)\n",
    "    print(\"count of Wav_files in {}= {}\".format(len(image_files),image_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
